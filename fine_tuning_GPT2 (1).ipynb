{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-requests"
      ],
      "metadata": {
        "id": "k0PiB7VHSH2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the books of the A Song of Ice and Fire series from the internet\n",
        "\n",
        "\n",
        "1.   A Game of Thrones\n",
        "2.   A Clash of Kings\n",
        "3.   A Storm of Swords\n",
        "4.   A Feast for Crows\n",
        "5.   A Dance with Dragons\n",
        "\n"
      ],
      "metadata": {
        "id": "narTx1UjSLT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Packages Installations"
      ],
      "metadata": {
        "id": "-IDzXw7YQsDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   PyPDF2 - to read from pdf files\n",
        "2.   tiktoken - to tokenize text\n",
        "3.   openai - to generate questions and answers from a paragraph\n",
        "4.   backoff - to retry a function in case of an exception"
      ],
      "metadata": {
        "id": "OnQaQCvlQwnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 tiktoken openai backoff\n"
      ],
      "metadata": {
        "id": "IAKkGSF8QwZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Home directory setup"
      ],
      "metadata": {
        "id": "bxXYNAOaTUwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import os"
      ],
      "metadata": {
        "id": "oJEqDg1gorN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import os\n",
        "os.chdir(\"/path/to/the/project/home/dir\")"
      ],
      "metadata": {
        "id": "62egJVGlTSGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PDF to TXT"
      ],
      "metadata": {
        "id": "RZfFcaUMRusV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting pdf files into text files and returing their content as a string"
      ],
      "metadata": {
        "id": "azBZfv-dR-_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B39xIo4cQXQx"
      },
      "outputs": [],
      "source": [
        "\"converting pdf files to text files, and returning one string contains all the files content\"\n",
        "\n",
        "# function that gets pdf file path , and destination dir, convert the pdf into txt file and save it in the destination path\n",
        "def convert_pdf_to_txt(pdf_path, destination_path):\n",
        "    # convert the pdf into txt file\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for i, page in enumerate(pdf_reader.pages):\n",
        "        text += page.extract_text()\n",
        "    # save the text to the destination path\n",
        "    with open(destination_path, \"w\") as file:\n",
        "        file.write(text)\n",
        "    return text\n",
        "\n",
        "# function gets pdf files directory, and destination dir, convert the pdf files , one by one into txt file and save it in the destination path\n",
        "def convert_pdf_files_to_txt(pdf_files_dir, destination_dir):\n",
        "    # create the destination dir if it doesn't exist\n",
        "    if not os.path.exists(destination_dir):\n",
        "        os.makedirs(destination_dir)\n",
        "\n",
        "    # convert the pdf files to txt files\n",
        "    for pdf_file in os.listdir(pdf_files_dir):\n",
        "        pdf_path = os.path.join(pdf_files_dir, pdf_file)\n",
        "\n",
        "        # check if the file is already converted to txt\n",
        "        if os.path.exists(os.path.join(destination_dir, pdf_file.replace(\".pdf\", \".txt\"))):\n",
        "            print(f\"File {pdf_file} already converted to txt\")\n",
        "            continue\n",
        "\n",
        "        txt_path = os.path.join(destination_dir, pdf_file.replace(\".pdf\", \".txt\"))\n",
        "        convert_pdf_to_txt(pdf_path, txt_path)\n",
        "\n",
        "\n",
        "# function that takes txt files directory path, and read all the txt files and return them as a string\n",
        "def get_text_data(txt_files_dir = \"data\", pdf_files_dir = \"data_pdf\"):\n",
        "    # convert the pdf files to txt files\n",
        "    convert_pdf_files_to_txt(pdf_files_dir, txt_files_dir)\n",
        "\n",
        "    text = \"\"\n",
        "    for txt_file in os.listdir(txt_files_dir):\n",
        "        txt_path = os.path.join(txt_files_dir, txt_file)\n",
        "        with open(txt_path, \"r\") as file:\n",
        "            text += file.read()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "books_text = get_text_data(txt_files_dir = \"data\", pdf_files_dir = \"data_pdf\")"
      ],
      "metadata": {
        "id": "zerL5HWfTuG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting the string into chunks with size to be 200 chars"
      ],
      "metadata": {
        "id": "IaU89DqZUcly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 1500\n",
        "chunks=[]\n",
        "\n",
        "for i in range(1000,len(books_text)-1000, chunk_size-200):\n",
        "  chunks.append(books_text[i:i+chunk_size].replace(\"\\n\", \" \").replace(\"\\t\",\" \"))"
      ],
      "metadata": {
        "id": "EDKQUdd0UYe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating questions and answers"
      ],
      "metadata": {
        "id": "N47hjmPCT0fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Get your api key from openai platform"
      ],
      "metadata": {
        "id": "_RbbLyT7T6fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY=\"your api key\""
      ],
      "metadata": {
        "id": "O7Si7ARvTuwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating the QA sets"
      ],
      "metadata": {
        "id": "mEEBCcqtUKq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "qa_batch.py  ‚Äì  Generate Q/A pairs for a list of passages.\n",
        "\n",
        "‚Ä¢ If `use_batch=True` and you have the Batch API quota enabled,\n",
        "  the helper will upload a JSONL file, run the job at ‚Äì50‚ÄØ% cost,\n",
        "  and poll until the results are ready.\n",
        "\n",
        "‚Ä¢ Otherwise it falls back to (rate‚Äëlimited) per‚Äëchunk requests.\n",
        "\n",
        "Requires:  `pip install openai tiktoken backoff`\n",
        "           export OPENAI_API_KEY=sk‚Äë...\n",
        "\n",
        "¬©¬†2025.  Feel free to adapt / extend.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import json, os, tempfile, time, asyncio, backoff\n",
        "from typing import List, Dict, Union\n",
        "from openai import OpenAI, RateLimitError, APIStatusError\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "SYSTEM = \"You are a meticulous literary analyst. When answering, always provide full, informative, standalone sentences using clear language.\"\n",
        "TEMPLATE = \"\"\"<chunk>\n",
        "{chunk}\n",
        "</chunk>\n",
        "\n",
        "Write ONE fact‚Äëbased Q&A pair based strictly on the information in the chunk.\n",
        "\n",
        "‚Ä¢ Use JSON: {{ \"q\": \"...\", \"a\": \"...\" }}\n",
        "‚Ä¢ The **answer** must be a **complete and informative sentence** that can stand alone‚Äî\n",
        "  start with a capital letter and end with a period.\n",
        "‚Ä¢ The **answer** should contain enough context so that it makes sense **even without the question**, and should be at least **20 words long**, if possible.\n",
        "‚Ä¢ Do NOT invent facts. Stay within what is clearly stated or strongly implied in the chunk.\n",
        "\"\"\"\n",
        "\n",
        "# ---------- Low‚Äëlevel single request ---------- #\n",
        "@backoff.on_exception(backoff.expo, (RateLimitError, APIStatusError), max_tries=5)\n",
        "def _qa_for_chunk(chunk: str,\n",
        "                  model: str = \"gpt-4o-mini\",\n",
        "                  temperature: float = 0.4) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Call Chat Completions once and parse the {\"q\": \"...\", \"a\": \"...\"} JSON.\n",
        "    \"\"\"\n",
        "    prompt = TEMPLATE.format(chunk=chunk)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[{\"role\": \"system\", \"content\": SYSTEM},\n",
        "                  {\"role\": \"user\",   \"content\": prompt}],\n",
        "        max_tokens=120,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return json.loads(resp.choices[0].message.content)\n",
        "\n",
        "\n",
        "# ---------- Batch‚ÄëAPI helper ---------- #\n",
        "def _batch_submit(chunks: List[str],\n",
        "                  n: int = 1,\n",
        "                  model: str = \"gpt-4o-mini\") -> str:\n",
        "    \"\"\"\n",
        "    1. Create a temporary JSONL with one Chat‚ÄëCompletion task per chunk\n",
        "    2. Upload as an OpenAI Batch job (‚Äì50‚ÄØ% cost, 24‚ÄØh SLA)\n",
        "    3. Return the batch job ID\n",
        "    \"\"\"\n",
        "    fd, path = tempfile.mkstemp(suffix=\".jsonl\", text=True)\n",
        "    with os.fdopen(fd, \"w\") as f:\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            body = {\n",
        "                \"model\": model,\n",
        "                \"response_format\": {\"type\": \"json_object\"},\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM},\n",
        "                    {\"role\": \"user\",   \"content\": TEMPLATE.format(chunk=chunk)}\n",
        "                ],\n",
        "                \"max_tokens\": 160,\n",
        "                \"temperature\": 0.4\n",
        "            }\n",
        "            task = {\"custom_id\": f\"ck_{i}\", \"method\": \"POST\",\n",
        "                    \"url\": \"/v1/chat/completions\", \"body\": body}\n",
        "            f.write(json.dumps(task) + \"\\n\")\n",
        "\n",
        "    batch = client.batches.create(\n",
        "        input_file=open(path, \"rb\"),\n",
        "        endpoint=\"/v1/chat/completions\",\n",
        "        completion_window=\"24h\"\n",
        "    )\n",
        "    return batch.id\n",
        "\n",
        "\n",
        "def _batch_poll(batch_id: str, interval: int = 30) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Polls until the batch finishes, then downloads and parses the results.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        status = client.batches.retrieve(batch_id)\n",
        "        if status.status in (\"completed\", \"failed\", \"expired\"):\n",
        "            break\n",
        "        time.sleep(interval)\n",
        "\n",
        "    if status.status != \"completed\":\n",
        "        raise RuntimeError(f\"Batch {batch_id} ended with status={status.status}\")\n",
        "\n",
        "    # Download output file\n",
        "    output = client.files.content(status.output_file_id)\n",
        "    return [json.loads(line)[\"response\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "            for line in output.iter_lines() if line]\n",
        "\n",
        "\n",
        "# ---------- Public helper ---------- #\n",
        "def generate_qas(chunks: List[str],\n",
        "                 model: str = \"gpt-4o-mini\",\n",
        "                 use_batch: bool = True) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        chunks   ‚Äì List of text passages.\n",
        "        model    ‚Äì Chat model name (gpt‚Äë4o, gpt‚Äë4o‚Äëmini ‚Ä¶).\n",
        "        use_batch‚Äì True ‚Üí try Batch API; False ‚Üí synchronous fallback.\n",
        "\n",
        "    Returns:\n",
        "        List[dict] each like {'q': \"...\", 'a': \"...\"}\n",
        "    \"\"\"\n",
        "    if use_batch:\n",
        "        try:\n",
        "            batch_id = _batch_submit(chunks, model=model)\n",
        "            print(f\"Submitted Batch¬†{batch_id}. Waiting ‚Ä¶\")\n",
        "            raw_jsons = _batch_poll(batch_id)\n",
        "            return [json.loads(j) for j in raw_jsons]\n",
        "        except Exception as e:\n",
        "            print(f\"Batch failed¬†({e}). Falling back to per‚Äëchunk requests.\")\n",
        "\n",
        "    # fallback ‚Äì sequential (or you can wrap with asyncio/gather)\n",
        "    results = []\n",
        "    for ck in chunks:\n",
        "        results.append(_qa_for_chunk(ck, model=model))\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Hiz0F7dDUGsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we are generating the QA sets by sending batch of chunks to openai platform with a prompt to generate what we need"
      ],
      "metadata": {
        "id": "-UoKALyJVDdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "while we sending batches, we save the results after each 200 iterations for safty"
      ],
      "metadata": {
        "id": "aM0tD7mvVSbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amount_chunks=10\n",
        "qas=[]\n",
        "data=[]\n",
        "for i in range(0, len(chunks)-amount_chunks, amount_chunks):\n",
        "\n",
        "  qa = generate_qas(chunks[i:i+amount_chunks], model=\"gpt-4o-mini\", use_batch=True)\n",
        "  qas+=qa\n",
        "  if i%200 == 0:\n",
        "\n",
        "    #check if the file is found\n",
        "    if os.path.isfile(\"qa.json\"):\n",
        "      with open(\"qa.json\", \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    data+=qas\n",
        "    with open(\"qa.json\", \"w\") as f:\n",
        "      json.dump(data, f)\n",
        "    qas=[]\n",
        "    print(f\"done {i}\")\n",
        "\n",
        "with open(\"qa.json\", \"w\") as f:\n",
        "  json.dump(data, f)"
      ],
      "metadata": {
        "id": "9I_YVcmBUR4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading the generated data"
      ],
      "metadata": {
        "id": "jZ1u8mlsWNVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('qa.json', 'r', encoding='utf-8') as f:\n",
        "    qas = json.load(f)"
      ],
      "metadata": {
        "id": "1hmVQ3xCVe3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing"
      ],
      "metadata": {
        "id": "HI6lUlUfWa1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train/Test splitting"
      ],
      "metadata": {
        "id": "CMhDir3wWlNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we are splitting the data into trining and testing sets"
      ],
      "metadata": {
        "id": "SvXFG37CWsmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the books text separated from the questions and answers so the model can learn first the books and then learn how to answer questions"
      ],
      "metadata": {
        "id": "w1_BpMoDWyxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_qas_portion = int(len(qas)*0.98)\n",
        "test_qas_portion=int(len(qas)*0.02)\n",
        "\n",
        "train_data_qas = qas[:train_qas_portion]\n",
        "test_data_qas = qas[train_qas_portion:]\n",
        "\n",
        "train_txt_portion = int(len(chunks)*0.98)\n",
        "test_txt_portion=int(len(chunks)*0.02)\n",
        "\n",
        "train_data_txt = chunks[:train_txt_portion]\n",
        "test_data_txt = chunks[train_txt_portion:]\n",
        "\n",
        "print(f\"train_data_qas: {len(train_data_qas)}\")\n",
        "print(f\"test_data_qas: {len(test_data_qas)}\")\n",
        "print(f\"train_data_txt: {len(train_data_txt)}\")\n",
        "print(f\"test_data_txt: {len(test_data_txt)}\")"
      ],
      "metadata": {
        "id": "Ejb6UrCPWPw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALPACA format"
      ],
      "metadata": {
        "id": "dh3m1waRXcmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "its a formating questions and answers in a way the model can learn to answer a question"
      ],
      "metadata": {
        "id": "v-jOxxlDXgGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "  instruction_text=(\n",
        "      f\"Answer the following question with an appropriate response. \"\n",
        "      f\"Write a response that appropriately completes the qusetion.\"\n",
        "      f\"\\n\\n### question:\\n{entry['q']}\"\n",
        "  )\n",
        "  return instruction_text"
      ],
      "metadata": {
        "id": "1ZmFXrUAXFd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizer"
      ],
      "metadata": {
        "id": "D-r6gfslXt5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are using byte pair encoding and the gpt2 tokenizer"
      ],
      "metadata": {
        "id": "CuS8dL9mXv-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab size = 50257"
      ],
      "metadata": {
        "id": "9eFDnSZOX-WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "nBELzACEXtE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "769GCSn2ha4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this class tokenize the data and arranging it"
      ],
      "metadata": {
        "id": "R1vWZmvphhsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the data parameter contains the QA sets, so here we adding the answer to the formated question"
      ],
      "metadata": {
        "id": "nCuEck8sov75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the chunks are the strings from the books"
      ],
      "metadata": {
        "id": "IItPmeeKo_bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class qaDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data=[], chunks=[]):\n",
        "      '''\n",
        "      data: list of dictionaries with keys 'q' and 'a'\n",
        "      chunks: list of strings to be used as context\n",
        "      '''\n",
        "      self.qas = data\n",
        "\n",
        "      self.encoded_texts = []\n",
        "\n",
        "      for entry in data:\n",
        "        qa_text = format_input(entry)\n",
        "        response_text = f\"\\n\\n### Response:\\n{entry['a']}\"\n",
        "        full_text = qa_text + response_text\n",
        "        self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "      for chunk in chunks:\n",
        "        self.encoded_texts.append(tokenizer.encode(chunk))\n",
        "\n",
        "      random.shuffle(self.encoded_texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoded_texts[idx]"
      ],
      "metadata": {
        "id": "2OgWLHQrYMeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input/Target pairs"
      ],
      "metadata": {
        "id": "7_pbxH_Wp-jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function used while training, it gets input batch and arrange it to a input/target pair by making the target a shift to the right by one token, and also adding an end of text token and also after that the ignore tokens"
      ],
      "metadata": {
        "id": "sWHgGX_kq27h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch, pad_token_id=50256, device='cpu', ignore_index=-100, allowed_max_length=None):\n",
        "\n",
        "  '''\n",
        "  extracting max length\n",
        "\n",
        "  for each item\n",
        "    padding\n",
        "    input/target\n",
        "    -100 (ignore token)\n",
        "    appending\n",
        "\n",
        "  stacking to device\n",
        "  '''\n",
        "\n",
        "  batch_max_length=max(len(item)+1 for item in batch)\n",
        "\n",
        "  inputs_lst, targets_lst = [],[]\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "\n",
        "    new_item+=[pad_token_id]\n",
        "\n",
        "    # padding\n",
        "    padded = (\n",
        "        new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "    )\n",
        "\n",
        "    # inputs and targets\n",
        "    inputs = torch.tensor(padded[:-1])\n",
        "    targets = torch.tensor(padded[1:])\n",
        "\n",
        "    # mask with true values in places where the pad_token_id is found\n",
        "    mask = targets == pad_token_id # true or false in the places that meets the condition\n",
        "\n",
        "    # get indeces of places have ture values\n",
        "    indices = torch.nonzero(mask).squeeze()# indeces of places that none zero\n",
        "\n",
        "    # replace all pad_token_id but the first one\n",
        "    if indices.numel() > 1:\n",
        "      targets[indices[1:]] = ignore_index\n",
        "\n",
        "    # check if length is constrained\n",
        "    if allowed_max_length is not None:\n",
        "      inputs = inputs[:allowed_max_length]\n",
        "      targets = targets[:allowed_max_length]\n",
        "\n",
        "    # append\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "  #stack\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "  return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "MjD41KD7p1i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting up the device"
      ],
      "metadata": {
        "id": "TrQMPHoqrYtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "zft1Mu-UrTxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Testing sets"
      ],
      "metadata": {
        "id": "5rMaifSlrnPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "making a train and test sets for the text of the books and also sets for the QA sets"
      ],
      "metadata": {
        "id": "DyV3lsBurrTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "because we first train the model to learn the books content"
      ],
      "metadata": {
        "id": "9vK0ooSHr0ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we train the model to answer questions"
      ],
      "metadata": {
        "id": "W8ArS6-7r43x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)\n",
        "\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 4\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset_qas = qaDataset(tokenizer, train_data_qas, chunks)\n",
        "train_loader_qas = DataLoader(\n",
        "    train_dataset_qas,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_dataset_qas = qaDataset(tokenizer, test_data_qas)\n",
        "test_loader_qas = DataLoader(\n",
        "    test_dataset_qas,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "train_dataset_txt = qaDataset(tokenizer, [], train_data_txt)\n",
        "train_loader_txt = DataLoader(\n",
        "    train_dataset_txt,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_dataset_txt = qaDataset(tokenizer, [], test_data_txt)\n",
        "test_loader_txt = DataLoader(\n",
        "    test_dataset_txt,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "O8a5WN_4rdBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating Text"
      ],
      "metadata": {
        "id": "dQm6QhBhtrzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   this function is geting batch of texts (idx)\n",
        "*   pass them to the model\n",
        "*   gets the model output\n",
        "*   process the output\n",
        "*   return the generated text\n",
        "\n"
      ],
      "metadata": {
        "id": "RCL90HUEtyyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "temperature - is to use a variety of options instead of getting the same result for the same input"
      ],
      "metadata": {
        "id": "1m569zHauJHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    '''\n",
        "    iterate for max_new_tokens times\n",
        "    get the last context_size tokens of each sentence\n",
        "    inference\n",
        "    get last vector of each sentence output\n",
        "    topk: set others to -inf\n",
        "    temp: divide=>softmax=>moltinomial\n",
        "    if eos: stop generating\n",
        "    else: append to idx with cat\n",
        "\n",
        "    '''\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "eK9SoZESr-fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss"
      ],
      "metadata": {
        "id": "MEhCPIzqujiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cross entropy is for maximizing the correct result probabilities and minimizing the wrong results probabilities"
      ],
      "metadata": {
        "id": "O3aOxUEfuuEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* the output of the model is batch of matrixes, each matrix is output for each input, the output matrix consists of a vocab size vector for each token, and each cell of the vector represents the probabilty for that token to be next to the corresponting input token."
      ],
      "metadata": {
        "id": "N9hN3GtAu_kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* gets the probability of the target token"
      ],
      "metadata": {
        "id": "sXXqx-_rvvdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* sum their logs"
      ],
      "metadata": {
        "id": "LO4KDDAs0Y_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* divide by the amount of tokens (average)"
      ],
      "metadata": {
        "id": "kyZJ2yAL0eMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* multiply by -1"
      ],
      "metadata": {
        "id": "DhHvb7nK0quW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the number we got represents how far the result from the target"
      ],
      "metadata": {
        "id": "NpFd9KYl0yBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss"
      ],
      "metadata": {
        "id": "DMH2ELQsuciY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating the loss for a dataset"
      ],
      "metadata": {
        "id": "HoA9FOED-Iy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "O1c7AzOQurca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Structure"
      ],
      "metadata": {
        "id": "37R1KCq0-Yuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT"
      ],
      "metadata": {
        "id": "F_Q2Mylwo3Ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The journy a text takes toward generating the next word\n",
        "\n",
        "***--- input example: \"my name is karam and\"***\n",
        "<br>\n",
        "<br>\n",
        "1.   Text tokenization.\n",
        "      *   using byte pair to convert each token with token id\n",
        "      *   we add end of text token and then padding tokens.\n",
        "      *   end of text token: 50256\n",
        "      *   padding token: -100\n",
        "      *   result: [23, 43, 678, 3468, 4234, 50256, -100, -100, -100,... ]\n",
        "      *   length: 1024, we usually call it context_length.\n",
        "\n",
        "***--- shape = 1024 = input_size***\n",
        "<br>\n",
        "<br>\n",
        "2.   Token embedding:\n",
        "\n",
        "      *   lookup table, 50257X768 matrix, converting the token id into a vector that correspond to that id. like a dictionary, give me the word and ill give you the meaning, the word here is the token id and the meaning here is a vector of size 768\n",
        "\n",
        "      *   50257: is the vocab size\n",
        "\n",
        "      *   768: is a token embedding size\n",
        "\n",
        "      *   meaning: each word represented by a vector, words close in meaning have smaller distanses between each others.\n",
        "\n",
        "      *  result: 1024 vectors each one of length 768, shape=[1024,768]\n",
        "\n",
        "        [\n",
        "\n",
        "            [768 values],\n",
        "\n",
        "            [768 values],\n",
        "\n",
        "            [768 values],\n",
        "\n",
        "            [768 values],\n",
        "\n",
        "            [768 values],\n",
        "\n",
        "            .\n",
        "\n",
        "            .\n",
        "\n",
        "            .\n",
        "\n",
        "        ]\n",
        "\n",
        "***-- shape = [1024, 768] = [input_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "3.   Positional embedding.\n",
        "      *   lookup table, context_length √ó emb_dim matrix, converting the position index into a vector that corresponds to that position. Like a dictionary: give me the position (0, 1, 2 ‚Ä¶) and ill give you a vector that represents ‚Äúbeing in that place‚Äù.\n",
        "      \n",
        "      *   context_length: the maximum number of tokens the model can handle in one sequence (e.g., 1024). So there are 1024 rows ‚Äî one for each possible position.\n",
        "\n",
        "      *   emb_dim: the embedding size (same 768 as the token embeddings).\n",
        "\n",
        "      *   meaning: each position in the sequence has its own learned vector Position 0 has one vector, position 1 another, ‚Ä¶ . These vectors inject the idea of order, so the model knows which token comes first, second, third, etc.\n",
        "\n",
        "      *   result: if the input has 5 tokens, you‚Äôll get 5 vectors (one for each position: 0 to 4), each vector of length 768, shape = [1024, 768]\n",
        "\n",
        "      [\n",
        "\n",
        "        [768 values for position 0],\n",
        "\n",
        "        [768 values for position 1],\n",
        "\n",
        "        [768 values for position 2],\n",
        "\n",
        "        [768 values for position 3],\n",
        "\n",
        "        [768 values for position 4],\n",
        "\n",
        "        .\n",
        "\n",
        "        .\n",
        "\n",
        "        .\n",
        "\n",
        "      ]\n",
        "***-- shape = [1024, 768] = [input_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "***Token and positional embeddings are added element-wise, so each vector represents both the word‚Äôs meaning and its position in the sequence.***\n",
        "<br>\n",
        "<br>\n",
        "***-- shape=[1024,768]***\n",
        "<br>\n",
        "<br>\n",
        "4.   n transformer blocks .\n",
        "      *   [explained later.]\n",
        "\n",
        "***-- shape = [1024, 768] = [input_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "5.   Normalization.\n",
        "      *   keeping the mean to stay close to 0 and the variance to be close to 1.\n",
        "\n",
        "      *   goal: preventing overflow errors and the numbers to become inf.\n",
        "\n",
        "***-- shape = [1024, 768] = [input_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "6.   Output layer.\n",
        "      *   layer that shapes the output into the shape of input_length X 50257. that means for each input word there 50257 value, we call the 50257 by voccab size.\n",
        "\n",
        "      *   50257: those are the amount of words in the dictionary, each cell contains the probability of that word to be the next word.\n",
        "\n",
        "      *   layer matrix shape: embedding_dim X vocab_size , (embedding_dim is the token vector size and vocab_size is the amount of tokens in a dictionary)\n",
        "\n",
        "      *   input matrix size: input_size X embedding_dim (input_size is the length of the input to the model which means amount of tokens)\n",
        "\n",
        "      *   result matrix input_length X vocab_size\n",
        "<br>\n",
        "<br>\n",
        "***-- shape  = [1024, 50257] = [input_length, vocab_size]***\n"
      ],
      "metadata": {
        "id": "cKDqYWa_mri4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer"
      ],
      "metadata": {
        "id": "BeyL-b_D1yrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer structure\n",
        "\n",
        "\n",
        "\n",
        "1.   layer norm 1.\n",
        "      *   the same as the Normalization layer explained earlier.\n",
        "\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "2.   multi-head attention.\n",
        "      *   [explained later]\n",
        "\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "3.   dropout.\n",
        "      *   the concept is that we turn of a fraction of the learned parameters for avoiding overfitting.\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "4.   residual connection.\n",
        "      *   its adding the values of the transformer input to the dropout output.\n",
        "      *   had a huge effect when dealing with a deep network, first used in ALEXNET.\n",
        "\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "5.   layer norm 2.\n",
        "      *   the same as the Normalization layer explained earlier.\n",
        "\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "6.   feed forward layer.\n",
        "      *   it have 2 dense layers\n",
        "      *   input to first layer: [1024, 768] which is [context_length, emb_dim]\n",
        "      *   params of first layer: [768, 3072] which is [emb_dim, 4*emb_dim]\n",
        "      *   output of first layer: [1024, 3072] which is [context_length, 4*emb_dim]\n",
        "      *   GELU activation: [1024, 3072] => [1024, 3072] keep the same size\n",
        "      *   input to second layer [1024, 3072] which is [context_length, 4*emb_dim]\n",
        "      *   params of second layer: [3072, 768] which is [4*emb_dim, emb_dim]\n",
        "      *   output of second layer: [1024, 768] which is [context_length, emb_dim] which means the same of the feed forward layer input shape.\n",
        "\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "7.   dropout layer.\n",
        "      *   explained earlier.\n",
        "\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "8.   residual connection\n",
        "      *   its adding the values of the first dropout output to the second dropout output layers.\n",
        "\n",
        "\n",
        "***--shape=[1024, 768] = [context_length, emb_dim]***"
      ],
      "metadata": {
        "id": "EtK0Pnlv104z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##multi-head attention\n"
      ],
      "metadata": {
        "id": "XivvJyCRIC6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***--- input = [1024, 768] = [context_length, emb_dim]***\n",
        "<br>\n",
        "<br>\n",
        "1.   Linear projections (queries, keys, values)\n",
        "      *   Input: [1024, 768] = [context_length, emb_dim]\n",
        "      *   Parameters: three matrices of shape [768, 768] (for Q, K, V).\n",
        "      *   Each token embedding is projected into three spaces:\n",
        "            *   Query (Q): what am I looking for?\n",
        "            *   Key (K): what do I contain?\n",
        "            *   Value (V): what information do I pass on if selected?\n",
        "      *   Result:\n",
        "            *   Q: [1024, 768]\n",
        "            *   K: [1024, 768]\n",
        "            *   V: [1024, 768]\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "2.   Split into heads\n",
        "      *   d_out must be divisible by num_heads\n",
        "      *   Suppose num_heads = 12\n",
        "      *   head_dim = d_out/num_heads = 768 / 12 = 64.\n",
        "      *   Reshape each Q, K, V into [1024, 12, 64].\n",
        "      *   transpose the 0 and 1 dims, so instead for each token there is 12 heads we look at it as each head have 1024 tokens.\n",
        "      *   Now each head can focus on different relationships independently.\n",
        "      *   Result:\n",
        "            *   Q output: [12, 1024, 64]\n",
        "            *   K output: [12, 1024, 64]\n",
        "            *   V output: [12, 1024, 64]\n",
        "            *   means: [num_heads, num_tokens, head_dim]\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "3.   Attention scores (scaled dot-product)\n",
        "      *   Q, V each have 12 heads, and each head have 1024 token vectors.\n",
        "      *   For each head, compute:\n",
        "\n",
        "            *   scores = (ùëÑ‚ãÖùêæ.ùëá)/sqrt(head_dim)\n",
        "                  *   head_dim=64\n",
        "                  *   K.T is transposing the last 2 dims of the key resulted matrix so we can compute the score for each head\n",
        "      *   This gives a similarity score between every pair of tokens.\n",
        "      *   Shape per head: [1024, 1024] (every token attends to every token).\n",
        "      *   result:\n",
        "            *   score shape: [12, 1024, 1024]\n",
        "            *   and we still have V output: [12, 1024, 64] as it is\n",
        "\n",
        "      *   A causal mask is applied so each position can only look backward (no cheating by looking at future tokens).\n",
        "      *   softmax: Softmax turns the attention scores into simple weights that add up to 1, so each word knows how much to pay attention to the others.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "4.   Context vector for each token\n",
        "      *   score shape: [12, 1024, 1024]\n",
        "      *   and we still have V output: [12, 1024, 64] as it is.\n",
        "      *   12 is number of heads\n",
        "      *   For each head, compute:\n",
        "            *   score‚ãÖV  # means [1024, 1024]‚ãÖ[1024, 64]\n",
        "      *   result: [12, 1024, 64]\n",
        "      *   transpose [12, 1024] to [1024, 12] # first two dims in result\n",
        "      *   result: [1024, 12, 64]\n",
        "      *   merge last 2 dims in result => [1024, 768]\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "***-- output shape: [1024, 768]***"
      ],
      "metadata": {
        "id": "hFZudkGrII1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT Code"
      ],
      "metadata": {
        "id": "Tj1OvC4Ajdb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 classes to build the model all explained above:\n",
        "\n",
        "\n",
        "1.   LayerNorm\n",
        "2.   GELU\n",
        "3.   FeedForward\n",
        "4.   MultiHeadAttention\n",
        "5.   TransformerBlock\n",
        "6.   GPTModel\n",
        "\n"
      ],
      "metadata": {
        "id": "lZ5XtHFujfWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # the dims hear are just to make sure they have the same dims\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)# replace in attn_scores: where is 1 in mask, put -inf in attn_scores\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # contiguous: to make sure the matrix in the same memory block, biew=reshape\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "0bwd0dlm-NDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot train and test loss"
      ],
      "metadata": {
        "id": "8bYMut52kDKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "euVVPy2q1xTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "aIeEwOKXkP3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "vt_2npiNkR1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text"
      ],
      "metadata": {
        "id": "TCZTYFUbkSvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokens <=> text"
      ],
      "metadata": {
        "id": "AIeZGBx2obLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "VSSTHLa0niWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate number of tokens"
      ],
      "metadata": {
        "id": "-oqJLkcnogye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    '''\n",
        "    iterate for max_new_tokens times\n",
        "    get the last context_size tokens of each sentence\n",
        "    inference\n",
        "    get last vector of each sentence output\n",
        "    topk: set others to -inf\n",
        "    temp: divide=>softmax=>moltinomial\n",
        "    if eos: stop generating\n",
        "    else: append to idx with cat\n",
        "\n",
        "    '''\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "xiGcHahunSvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Print the generated text"
      ],
      "metadata": {
        "id": "a89isSzDorUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "k_OqSUCHkgTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download & Load trained weights"
      ],
      "metadata": {
        "id": "tIO7xuD1ToJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download trained weights"
      ],
      "metadata": {
        "id": "yhvGxRQLpFXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download trained weights from kaggle and organize the weights in the params dictionary (explained later) to embedd them later in the model."
      ],
      "metadata": {
        "id": "U0v-gmYoRWBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests  # Make sure requests is installed\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "# downloading the seven files from caggle\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path)\n",
        "\n",
        "    ## We have reached here until now ---> we have downloaded the files on our local machine.\n",
        "\n",
        "    # Load settings and params\n",
        "\n",
        "    # get the path of the model weights\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "    # loading the model settings\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "def download_file(url, destination):\n",
        "    try:\n",
        "        # Send a GET request to download the file, disabling SSL verification\n",
        "        response = requests.get(url, stream=True, verify=False)\n",
        "\n",
        "        # Get the total file size from headers, defaulting to 0 if not present\n",
        "        file_size = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "        # Check if file exists and has the same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return\n",
        "\n",
        "        # Define the block size for reading the file\n",
        "        block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "        # Initialize the progress bar with total file size\n",
        "        progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "            # Open the destination file in binary write mode\n",
        "            with open(destination, \"wb\") as file:\n",
        "                # Iterate over the file data in chunks\n",
        "                for chunk in response.iter_content(block_size):\n",
        "                    progress_bar.update(len(chunk))  # Update progress bar\n",
        "                    file.write(chunk)  # Write the chunk to the file\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "        print(f\"Please check the URL: {url}\")\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "AhcEvT3GpKGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Configurations setup"
      ],
      "metadata": {
        "id": "huDROXfASk-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.05,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "id": "U_2nEFY7Sadn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading weights"
      ],
      "metadata": {
        "id": "8l9YsEk8Txvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layer[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layer[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layer[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layer[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layer[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layer[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layer[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layer[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "FiwyBklOSqtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##params dictionary keys explained"
      ],
      "metadata": {
        "id": "Rs8eCDxJTcG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[\"wte\"]: token embeddings\n",
        "the embedding layer in the begenning of the transformer block, it have trainable weights\n",
        "50257 X 768\n",
        "the input is context length, which is the tokenized sentence that passed to the model, which is 1024, its passed throught the embedding matrix and resulting a 1024x768 matrix\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "[\"wpe\"]: positional embeddings\n",
        "the positional embedding layer, with trainable weights 1024x768 matrix that we add it to the embedding resulted matrix\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "\n",
        "[\"blocks\"]: transformers\n",
        " - Q,K,V wieghts\n",
        "  - 1- [\"transformer/h0/attn/c_attn/w\"]\n",
        "  - 2- [\"transformer/h0/attn/c_attn/b \"]\n",
        "  - c_attn is the combined kqv params\n",
        "\n",
        " - feed forward wieghts\n",
        "  - first layer\n",
        "    - [\"transformer/h0/mlp/c_fc/w\"]\n",
        "    - [\"transformer/h0/mlp/c_fc/b \"]\n",
        "  - second layer\n",
        "    - [\"transformer/h0/c_proj/c_fc/w\"]\n",
        "    - [\"transformer/h0/c_proj/c_fc/b \"]\n",
        "  - output projection layer\n",
        "    - [\"transformer/h0/c_proj/w\"]\n",
        "    - [\"transformer/h0/c_proj/b \"]\n",
        "\n",
        "\n",
        " - normalization layers\n",
        "  - first layer\n",
        "    - [\"transformer/h0/ln_1/g\"]\n",
        "    - [\"transformer/h0/ln_1/b \"]\n",
        "  - second layer\n",
        "    - [\"transformer/h0/ln_2/g\"]\n",
        "    - [\"transformer/h0/ln_2/b \"]\n",
        "---------------------------------\n",
        "\n",
        "[\"g\"]: final norm scale (w)\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "[\"b\"]: final norm shift (b)"
      ],
      "metadata": {
        "id": "Jmp8M3v9S8fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model setup"
      ],
      "metadata": {
        "id": "4N94iLqYUEET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creat and load"
      ],
      "metadata": {
        "id": "gHLMGAK7UUaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "2zEhX8huS89y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4jpH2GQAUR-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing a small sample"
      ],
      "metadata": {
        "id": "SNDboR4lUZDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Hello, how are you?\"\n",
        "token_ids = text_to_token_ids(input_text, tokenizer)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model.to(device),\n",
        "    idx=token_ids.to(device),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "id": "YqqBZSy_URQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tuning Time"
      ],
      "metadata": {
        "id": "pILC41zbU4T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training function"
      ],
      "metadata": {
        "id": "TMcGlbHXU_Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer, checkpoints_dir):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "    all_val_loss = []\n",
        "\n",
        "    #if checkpoints directory doesnt exists, create it\n",
        "    if not os.path.exists(checkpoints_dir):\n",
        "        os.makedirs(checkpoints_dir)\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, \"you know nothing\"#\"start_context\"\n",
        "        )\n",
        "\n",
        "        # Save the model after each epoch\n",
        "        model_path = f\"{checkpoints_dir}/model_epoch_{epoch+1}.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            full_loss = calc_loss_loader(val_loader, model, device)\n",
        "        model.train()\n",
        "\n",
        "        all_val_loss.append(full_loss)\n",
        "\n",
        "        print(f\"\\n\\n\\n===============Epoch {epoch+1} val loss: {full_loss}\")\n",
        "        if epoch > 5:\n",
        "            # early stopping when validation loss is more than 3 validation losses\n",
        "            sorted_val_losses = all_val_loss.copy()\n",
        "            sorted_val_losses.sort()\n",
        "            min_5_val_losses = sorted_val_losses[:5]\n",
        "            if full_loss > max(min_5_val_losses):\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # save the name of the best model\n",
        "    index_of_best_model = all_val_loss.index(min(all_val_loss))\n",
        "    best_model_path = f\"{checkpoints_dir}/model_epoch_{index_of_best_model+1}.pth\"\n",
        "    with open(f\"{checkpoints_dir}/best_model_name.json\", \"w\") as f:\n",
        "        json.dump({\"model_path\": best_model_path}, f)\n",
        "\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "siZkUAl7Uk4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning"
      ],
      "metadata": {
        "id": "8Qp02aj0VL3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first we fine-tune the model on the books content and later on the QA sets"
      ],
      "metadata": {
        "id": "KdEfit7AWeyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "print(f\"Train loader length: {len(train_loader_txt)}\")\n",
        "print(f\"Val loader length: {len(test_loader_txt)}\")\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "   device = torch.device(\"mps\")\n",
        "else:\n",
        "   device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader_txt, model, device)\n",
        "    val_loss = calc_loss_loader(test_loader_txt, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to calculate the execution time\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4, weight_decay=0.01)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "num_epochs = 100\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader_txt, test_loader_txt, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=20,\n",
        "    start_context=\"who said you know nothing jon snow?\", tokenizer=tokenizer, checkpoints_dir=\"checkpoints_txt\"\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
        "\n",
        "# load the best model\n",
        "with open(\"checkpoints_txt/best_model_name.json\", \"r\") as f:\n",
        "    best_model_path = json.load(f)[\"model_path\"]\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "2cb4Vi2NVOLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch\n",
        "\n",
        "# 1) Drop references to big objects\n",
        "objs = [\"model\",\"optimizer\",\"scaler\",\"train_loader\",\"val_loader\",\"batch\",\"loss\",\"outputs\",\"inputs\",\"targets\"]\n",
        "for name in objs:\n",
        "    if name in globals(): del globals()[name]"
      ],
      "metadata": {
        "id": "54jdo5yV1-l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##fine-tune model on QA sets"
      ],
      "metadata": {
        "id": "08Uyn5Vneija"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader_qas, test_loader_qas, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=20,\n",
        "    start_context=\"who said you know nothing jon snow?\", tokenizer=tokenizer, checkpoints_dir=\"checkpoints_qas\"\n",
        ")"
      ],
      "metadata": {
        "id": "bJi55xzXemmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##plotting losses"
      ],
      "metadata": {
        "id": "6f2YNvRyfZzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "5aRjm1MYfb9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the best model"
      ],
      "metadata": {
        "id": "BqqMf7F3ffgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best model\n",
        "with open(\"checkpoints_qas/best_model_name.json\", \"r\") as f:\n",
        "    best_model_path = json.load(f)[\"model_path\"]\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "EfzKbjDXfh_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}